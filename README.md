# ğŸ§  Yohan Lee (ìš”í•œ)
### AI Research Engineer @ KakaoBank | Building Reasoning Systems at Human & Model Scale

> _Designing language models that think, remember, and reason â€” like humans, but at scale._

---

## ğŸ§© About Me

Iâ€™m an **AI Research Engineer at KakaoBank**, focusing on **post-training and optimization of large language models** that power real-world services for **26M+ active users**.  
My work explores how **reasoning, memory, and alignment** can be reimagined to make LLMs both *cognitively inspired* and *production-ready*.

Rather than scaling parameters alone, I study how models can **think more like humans** â€”  
reasoning in parallel, integrating contextual memory, and aligning with human trust and intention.

---

## ğŸ”­ Research Focus

### ğŸ§  Human-like Reasoning
Current LLMs reason **sequentially**, generating one token at a time.  
Inspired by recent directions such as *Soft Token Reasoning* ([arXiv:2509.19170](https://arxiv.org/abs/2509.19170)),  
Iâ€™m exploring ways to enable **parallel and continuous inference** â€”  
models that can revise, aggregate, and evolve thoughts before producing answers.  
This connects symbolic reasoning with diffusion-like latent dynamics, aiming for *human-parallel cognition*.

### âš™ï¸ Scalable Reasoning Systems
At KakaoBank, I lead **post-training and inference optimization** for **200B+ parameter LLMs**,  
building high-impact reasoning agents in financial and service domains.  
My work centers on:
- **Interleaved reasoning** combining function calls, memory, and tool use  
- **Multi-instruction reasoning**, enabling one instruction to branch into multiple sub-tasks  
- **Latency-optimized alignment**, balancing inference speed with reasoning depth  

### ğŸ§¬ Memory & Cognitive Modeling
Following earlier work on **episodic and structured memory (PREMem, 2025)**,  
I study how models can construct and manage internal memory representations â€”  
learning to **consolidate**, **forget**, and **contextualize** experiences across sessions.  
The goal is a reasoning loop that grounds decisions in structured, evolving memory.

### ğŸ›¡ï¸ Trustworthy & Human-Aligned AI
Reasoning and memory must ultimately be *safe*.  
I develop and evaluate methods that ensure **consistency, transparency, and calibration** in model outputs â€”  
AI systems that *reflect before responding* and can *justify their reasoning processes*.  
This connects deeply to my broader pursuit: **aligning artificial reasoning with human cognition and ethics**.

---

## ğŸ† Publications

- **Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval**  
  *EMNLP 2025 (Industry Track)* â€” The first benchmark for conversational retrieval, exposing model weaknesses.  
  _Yohan Lee, Yongwoo Song, Sangyeop Kimâ€ _

- **PREMem: Pre-Storage Reasoning for Episodic Memory**  
  *EMNLP 2025 (Findings)* â€” Shifting reasoning to memory construction for personalized dialogue.  
  _Sangyeop Kim*, Yohan Lee*, Sanghwa Kim, Hyunjong Kim, Sungzoon Choâ€ _

- **What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs**  
  *ACL 2025 (Main)* â€” Revealing that context length, not shot count, drives long-context vulnerabilities.  
  _Sangyeop Kim*, Yohan Lee*, Yongwoo Song*, Kimin Leeâ€ _

- **HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using LLMs**  
  *NAACL 2025 (Findings)* â€” Training-free retrieval via hierarchical semantic indexing.  
  _Sangyeop Kimâ€ , Hangyeul Lee, Yohan Lee_

- **SAFARI: Sample-specific Assessment Framework for AI in Real-world Interactions**  
  *NAACL 2025 (Findings)* â€” Automated multilingual evaluation framework for LLMs using real-world conversational data.  
  _Yohan Lee*, Sungho Park*, Sangwoo Han*, Yunsung Lee*â€ , Yongwoo Song, Adam Lee, Jiwung Hyun, Jaemin Kim, HyeJin Gong_

---

## ğŸ“« Connect

ğŸ“ Seoul, South Korea  
ğŸŒ [Portfolio](https://l-yohai.github.io/portfolio/)  
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/l-yohai/)  
ğŸ“§ yhlee.nlp [at] gmail.com  

---

> _â€œAI should not only scale in size, but in understanding â€”  
reasoning with reflection, memory, and humanity.â€_
